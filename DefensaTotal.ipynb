{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA availability:\n",
      "CUDA available:  True\n",
      "Device count:  1\n",
      "CUDA device name:  NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Comenzando episodio 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_27512\\1153593641.py:96: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.tensor(states, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa del episodio : -333.0265017667845\n",
      "Comenzando episodio 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 200\u001b[0m\n\u001b[0;32m    197\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Llamar a la función de entrenamiento con 1000 episodios\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 173\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(episodes, max_steps, batch_size, gamma, epsilon_decay, epsilon_min, render)\u001b[0m\n\u001b[0;32m    171\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    172\u001b[0m     step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 173\u001b[0m     \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m#train_network se llama para entrenar q_network con un batch de experiencias de \u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# memory, ajustando los pesos de q_network en función de los valores objetivo\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Reducir epsilon gradualmente para favorecer la explotación\u001b[39;00m\n\u001b[0;32m    178\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(epsilon \u001b[38;5;241m*\u001b[39m epsilon_decay, epsilon_min)\n",
      "Cell \u001b[1;32mIn[1], line 96\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(q_network, target_network, memory, gamma, batch_size, optimizer)\u001b[0m\n\u001b[0;32m     93\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Convertir las transiciones en tensores para el entrenamiento\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     98\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rewards, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importación de las librerías necesarias para el entorno, manipulación de datos, redes neuronales y memoria\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# Ruta para guardar y cargar el modelo\n",
    "# Definir la ruta completa para guardar el modelo\n",
    "MODEL_PATH = \"C:/Users/pablo/OneDrive - usfx/Desktop/pruebasPythorch/Laboratorio6PLC/q_network_model.pth\"\n",
    "\n",
    "# Definir un conjunto de acciones discretas representativas para el entorno\n",
    "DISCRETE_ACTIONS = [\n",
    "    [-1.0, 0.5, 0.0],   # Giro fuerte izquierda con aceleración\n",
    "    [-0.5, 0.5, 0.0],   # Giro suave izquierda con aceleración\n",
    "    [0.0, 0.8, 0.0],    # Recto con mayor aceleración\n",
    "    [0.5, 0.5, 0.0],    # Giro suave derecha con aceleración\n",
    "    [1.0, 0.5, 0.0],    # Giro fuerte derecha con aceleración\n",
    "    [0.0, 0.0, 0.3]     # Frenado más fuerte\n",
    "]\n",
    "\n",
    "# Función para verificar la disponibilidad de CUDA en PyTorch\n",
    "def check_pytorch_cuda():\n",
    "    \"\"\"Check if PyTorch can access the GPU.\"\"\"\n",
    "    print(\"PyTorch CUDA availability:\")\n",
    "    print(\"CUDA available: \", torch.cuda.is_available())\n",
    "    print(\"Device count: \", torch.cuda.device_count())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device name: \", torch.cuda.get_device_name(0))\n",
    "\n",
    "check_pytorch_cuda()\n",
    "\n",
    "# Seleccionar el dispositivo CUDA si está disponible para el entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Definir la red Q con PyTorch para calcular los valores Q para cada acción\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # Definir capas convolucionales para extraer características de las observaciones del entorno\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Calcular el tamaño de la salida de la última capa convolucional para pasar a las capas densas\n",
    "        dummy_input = torch.zeros(1, *input_shape)\n",
    "        conv_out_size = self._get_conv_output(dummy_input)\n",
    "        \n",
    "        # Definir las capas densas para calcular el valor Q final para cada acción\n",
    "        self.fc1 = nn.Linear(conv_out_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    # Método para obtener el tamaño de salida después de las capas convolucionales\n",
    "    def _get_conv_output(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "    # Método para el paso hacia adelante (forward) en la red\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Aplanar la salida de la última capa convolucional\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Definir la función para seleccionar acciones usando la política epsilon-greedy\n",
    "def choose_action(state, q_network, epsilon):\n",
    "    # Selección de acción aleatoria para exploración\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(DISCRETE_ACTIONS)  # Exploración\n",
    "    else:\n",
    "        # Selección de la mejor acción estimada por la red para explotación\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action_idx = torch.argmax(q_values).item()\n",
    "            return DISCRETE_ACTIONS[action_idx]\n",
    "\n",
    "# Función para entrenar la red Q usando muestras de la memoria\n",
    "def train_network(q_network, target_network, memory, gamma, batch_size, optimizer):\n",
    "    # Asegurarse de que hay suficiente memoria para entrenar\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    # Seleccionar un batch aleatorio de la memoria\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    # Convertir las transiciones en tensores para el entrenamiento\n",
    "    states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor(dones, dtype=torch.bool, device=device)\n",
    "\n",
    "    # Calcular los valores Q actuales y los valores objetivo para la actualización\n",
    "    q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_network(next_states).max(1)[0]\n",
    "    targets = rewards + gamma * next_q_values * (~dones)\n",
    "\n",
    "    # Calcular el error cuadrático medio y actualizar la red\n",
    "    loss = nn.functional.mse_loss(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Definir la función principal de entrenamiento del agente\n",
    "def train(episodes, max_steps=500, batch_size=64, gamma=0.99, epsilon_decay=0.995, epsilon_min=0.01, render=False):\n",
    "    # Crear el entorno CarRacing en modo de renderizado\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode='human')\n",
    "    input_shape = (env.observation_space.shape[2],) + env.observation_space.shape[:2]\n",
    "    num_actions = len(DISCRETE_ACTIONS)\n",
    "    \n",
    "    # Crear las redes Q y de objetivo y sincronizar pesos\n",
    "    q_network = QNetwork(input_shape, num_actions).to(device)\n",
    "    target_network = QNetwork(input_shape, num_actions).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "    #Actualizar los pesos de q_network\n",
    "    epsilon = 1.0\n",
    "    memory = deque(maxlen=100000)\n",
    "    #deque para almacenar experiencias pasadas, con un tamaño máximo de 100,000\n",
    "    reward_per_episode = np.zeros(episodes)\n",
    "    #Arreglo para almacenar la recompensa total obtenida en cada episodio.\n",
    "    \n",
    "    # Bucle principal de entrenamiento por episodio\n",
    "    for episode in range(episodes):\n",
    "        if (episode+1) % 1 == 0:\n",
    "            env.close()\n",
    "            env = gym.make(\"CarRacing-v3\", render_mode=\"human\")\n",
    "        else:\n",
    "            env.close()\n",
    "            env = gym.make(\"CarRacing-v3\")\n",
    "\n",
    "        state, _ = env.reset(seed=42)\n",
    "        state = np.transpose(state, (2, 0, 1))\n",
    "        #(alto, ancho, canales) a (canales, alto, ancho)\n",
    "        episode_reward = 0\n",
    "        #recompensas acumuladas en cada episodio\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        #pasos o acciones\n",
    "        print(f\"Comenzando episodio {episode + 1}\")\n",
    "\n",
    "        # Bucle de pasos dentro de cada episodio\n",
    "        while not done and step_count < max_steps:\n",
    "            action = choose_action(state, q_network, epsilon)\n",
    "            action_idx = DISCRETE_ACTIONS.index(action)\n",
    "            next_state, reward, done, _, _ = env.step(np.array(action, dtype=np.float32))\n",
    "            #toma una accion y ejecuta un paso \n",
    "            \n",
    "            # Añadir recompensa si el agente acelera y penalizar giros fuertes\n",
    "            if action[1] > 0:\n",
    "                reward += 1.0  # Recompensa adicional por acelerar\n",
    "            if reward > 0 and action_idx == 2:\n",
    "                reward += 0.5\n",
    "            elif action_idx in [0, 4]:\n",
    "                reward -= 5\n",
    "\n",
    "            next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            # Reorganaizar las dimensiones de la observación del entorno\n",
    "            memory.append((state, action_idx, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            train_network(q_network, target_network, memory, gamma, batch_size, optimizer)\n",
    "            #train_network se llama para entrenar q_network con un batch de experiencias de \n",
    "            # memory, ajustando los pesos de q_network en función de los valores objetivo\n",
    "        \n",
    "        # Reducir epsilon gradualmente para favorecer la explotación\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        #Se reduce para que el agente explore menos con el tiempo\n",
    "        reward_per_episode[episode] = episode_reward\n",
    "\n",
    "        print( f\"Recompensa del episodio : {episode_reward}\")\n",
    "\n",
    "        # Actualizar los pesos de la red de objetivo cada 10 episodios\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "            #e actualiza copiando los pesos de q_network. \n",
    "            # Esto estabiliza el entrenamiento al reducir la varianza en los valores Q\n",
    "    \n",
    "    torch.save(q_network.state_dict(), MODEL_PATH)\n",
    "    print(f\"Modelo guardado en {MODEL_PATH}\")\n",
    "    # Graficar el desempeño del agente en términos de recompensa acumulada por episodio\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel(\"Episodios\")\n",
    "    plt.ylabel(\"Recompensa acumulada\")\n",
    "    plt.title(\"Desempeño del agente durante el entrenamiento\")\n",
    "    plt.show()\n",
    "\n",
    "# Llamar a la función de entrenamiento con 1000 episodios\n",
    "train(episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
