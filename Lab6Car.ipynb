{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_network(input_shape, action_space):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(32, 8, strides=4, activation='relu'),\n",
    "        layers.Conv2D(64, 4, strides=2, activation='relu'),\n",
    "        layers.Conv2D(64, 3, strides=1, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(action_space, activation='linear')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_network, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return action_space.sample()  # Exploración\n",
    "    else:\n",
    "        q_values = q_network.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        return q_values.flatten()  # Explotación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(q_network, target_network, memory, gamma, batch_size, optimizer):\n",
    "    if len(memory) < batch_size:\n",
    "        return  # No entrenar hasta que haya suficiente memoria\n",
    "\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "    states = np.array(states)\n",
    "    next_states = np.array(next_states)\n",
    "    \n",
    "    q_values = q_network.predict(states, verbose=0)\n",
    "    q_values_next = target_network.predict(next_states, verbose=0)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        target_q = rewards[i]\n",
    "        if not dones[i]:\n",
    "            target_q += gamma * np.max(q_values_next[i])\n",
    "        q_values[i] = q_values[i].flatten()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values_pred = q_network(states)\n",
    "        loss = tf.reduce_mean(tf.square(q_values_pred - q_values))\n",
    "    grads = tape.gradient(loss, q_network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_to_start(state, start_position):\n",
    "    # Asume que `state` y `start_position` son posiciones 2D (x, y) del agente en la pista\n",
    "    return np.linalg.norm(np.array(state[:2]) - np.array(start_position))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes, max_steps=1000, batch_size=64, gamma=0.99, epsilon_decay=0.995, epsilon_min=0.1, render=False):\n",
    "    # Inicialización del entorno y de los parámetros de entrenamiento\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode='human')\n",
    "    input_shape = env.observation_space.shape\n",
    "    action_space = env.action_space\n",
    "    \n",
    "    # Inicialización de las redes Q y objetivo\n",
    "    q_network = create_q_network(input_shape, action_space.shape[0])\n",
    "    target_network = create_q_network(input_shape, action_space.shape[0])\n",
    "    target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "    # Definición del optimizador y parámetros de epsilon para exploración\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    epsilon = 1.0  # Comenzar con alta exploración\n",
    "    memory = deque(maxlen=100000)  # Memoria de experiencias\n",
    "    reward_per_episode = np.zeros(episodes)\n",
    "\n",
    "    # Parámetros adicionales para el cálculo de vueltas y recompensas\n",
    "    proximity_threshold = 10.0  # Distancia para considerar que el agente ha completado una vuelta\n",
    "    lap_reward = 100  # Recompensa adicional por completar una vuelta\n",
    "    slow_progress_penalty = -10  # Penalización por progreso lento\n",
    "    min_progress_distance = 5  # Distancia mínima que debe recorrer en cada intervalo\n",
    "\n",
    "    # Ciclo de entrenamiento a través de los episodios\n",
    "    for episode in range(episodes):\n",
    "        # Reiniciar el entorno y establecer el punto inicial\n",
    "        state, _ = env.reset(seed=42)\n",
    "        initial_position = state[:2]  # Guardar la posición inicial del agente\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        last_position = initial_position  # Posición del último progreso registrado\n",
    "        progress_interval = max_steps // 10  # Intervalo para verificar progreso\n",
    "\n",
    "        print(f\"Comenzando episodio {episode + 1}\")\n",
    "\n",
    "        while not done and step_count < max_steps:\n",
    "            # Selección de acción usando la política epsilon-greedy\n",
    "            action = choose_action(state, q_network, epsilon, action_space)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "\n",
    "            # Calcular la distancia actual al punto de inicio\n",
    "            distance_to_start = calculate_distance_to_start(next_state, initial_position)\n",
    "\n",
    "            # Penalización adicional si el agente no ha avanzado en cierta distancia\n",
    "            if step_count % progress_interval == 0:\n",
    "                distance_covered = calculate_distance_to_start(next_state, last_position)\n",
    "                if distance_covered < min_progress_distance:\n",
    "                    reward += slow_progress_penalty  # Penalización por progreso lento\n",
    "                    print(\"Penalización por progreso lento aplicada.\")\n",
    "                last_position = next_state[:2]  # Actualizar última posición de progreso\n",
    "\n",
    "            # Aplicar recompensa adicional si el agente completa una vuelta\n",
    "            if distance_to_start < proximity_threshold and step_count > max_steps // 2:\n",
    "                reward += lap_reward\n",
    "                print(\"¡Vuelta completada! Recompensa adicional otorgada.\")\n",
    "            \n",
    "            # Almacenar la transición en la memoria\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Entrenamiento de la red Q usando experiencias almacenadas en la memoria\n",
    "            train_network(q_network, target_network, memory, gamma, batch_size, optimizer)\n",
    "\n",
    "            # Finalizar el episodio si la recompensa acumulada es muy negativa\n",
    "            if episode_reward < -100:\n",
    "                done = True\n",
    "                print(f\"Episodio {episode + 1} terminado anticipadamente por baja recompensa acumulada.\")\n",
    "        \n",
    "        # Reducir epsilon después de cada episodio para disminuir exploración gradualmente\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        # Almacenar la recompensa total del episodio\n",
    "        reward_per_episode[episode] = episode_reward\n",
    "\n",
    "        # Sincronizar la red objetivo periódicamente\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "        # Mostrar progreso cada 100 episodios\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episodio: {episode + 1} - Recompensa acumulada: {reward_per_episode[episode]}\")\n",
    "\n",
    "    # Graficar la recompensa acumulada por episodio al finalizar el entrenamiento\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel(\"Episodios\")\n",
    "    plt.ylabel(\"Recompensa acumulada\")\n",
    "    plt.title(\"Desempeño del agente durante el entrenamiento\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluación del agente entrenado\n",
    "    evaluate_agent(env, q_network)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, q_network, max_steps=1000):\n",
    "    state, _ = env.reset(seed=42)\n",
    "    state = np.array(state, dtype=np.float32)\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done and step_count < max_steps:\n",
    "        action = q_network.predict(np.expand_dims(state, axis=0), verbose=0).flatten()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Recompensa total en evaluación: {total_reward}\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando episodio 1\n",
      "Penalización por progreso lento aplicada.\n",
      "Entrenamiento interrumpido manualmente.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(episodes=100)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Entrenamiento interrumpido manualmente.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
