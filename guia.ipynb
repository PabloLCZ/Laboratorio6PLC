{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA availability:\n",
      "CUDA available:  False\n",
      "Device count:  0\n",
      "Comenzando episodio 1\n",
      "Recompensa del episodio : -269.49293286218943\n",
      "Comenzando episodio 2\n",
      "Recompensa del episodio : -339.895759717315\n",
      "Comenzando episodio 3\n",
      "Recompensa del episodio : -236.19151943462825\n",
      "Comenzando episodio 4\n",
      "Recompensa del episodio : -264.7964664310949\n",
      "Comenzando episodio 5\n"
     ]
    }
   ],
   "source": [
    "# Importación de las librerías necesarias para el entorno, manipulación de datos, redes neuronales y memoria\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Definir un conjunto de acciones discretas representativas para el entorno\n",
    "DISCRETE_ACTIONS = [\n",
    "    [-1.0, 0.5, 0.0],   # Giro fuerte izquierda con aceleración\n",
    "    [-0.5, 0.5, 0.0],   # Giro suave izquierda con aceleración\n",
    "    [0.0, 0.8, 0.0],    # Recto con mayor aceleración\n",
    "    [0.5, 0.5, 0.0],    # Giro suave derecha con aceleración\n",
    "    [1.0, 0.5, 0.0],    # Giro fuerte derecha con aceleración\n",
    "    [0.0, 0.0, 0.3]     # Frenado más fuerte\n",
    "]\n",
    "\n",
    "# Función para verificar la disponibilidad de CUDA en PyTorch\n",
    "def check_pytorch_cuda():\n",
    "    \"\"\"Check if PyTorch can access the GPU.\"\"\"\n",
    "    print(\"PyTorch CUDA availability:\")\n",
    "    print(\"CUDA available: \", torch.cuda.is_available())\n",
    "    print(\"Device count: \", torch.cuda.device_count())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device name: \", torch.cuda.get_device_name(0))\n",
    "\n",
    "check_pytorch_cuda()\n",
    "\n",
    "# Seleccionar el dispositivo CUDA si está disponible para el entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Definir la red Q con PyTorch para calcular los valores Q para cada acción\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # Definir capas convolucionales para extraer características de las observaciones del entorno\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        #32 salidas, num canales\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # 32 entradas y 64 salidas 4*4 y paso 2 px\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        #64 y 64, 3*3 y paso 1 px\n",
    "\n",
    "        # Calcular el tamaño de la salida de la última capa convolucional para pasar a las capas densas\n",
    "        dummy_input = torch.zeros(1, *input_shape)\n",
    "        #Al pasarlo a través de las capas convolucionales, obtenemos la cantidad exacta de características que resultan,\n",
    "        conv_out_size = self._get_conv_output(dummy_input)\n",
    "        \n",
    "        # Definir las capas densas para calcular el valor Q final para cada acción\n",
    "        self.fc1 = nn.Linear(conv_out_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    # Método para obtener el tamaño de salida después de las capas convolucionales\n",
    "    def _get_conv_output(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "    # Método para el paso hacia adelante (forward) en la red\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Aplanar la salida de la última capa convolucional\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Este proceso se basa en la política epsilon-greedy, que permite al agente equilibrar entre explorar nuevas acciones y explotar\n",
    "def choose_action(state, q_network, epsilon):\n",
    "    #state:parametro Actual\n",
    "    # q_network: red neuronal para estimar los valores Q\n",
    "    # epsilon: probabilidad de exploración para seleccionar una acción aleatoria\n",
    "    #epsilon alto favorece la exploración, mientras que un valor bajo favorece la explotación\n",
    "    # Selección de acción aleatoria para exploración\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(DISCRETE_ACTIONS)  # Exploración\n",
    "    else:\n",
    "        # Selección de la mejor acción estimada por la red para explotación\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            #Esto es necesario porque las redes neuronales de PyTorch suelen esperar un lote de datos de entrada.\n",
    "            q_values = q_network(state_tensor)\n",
    "            #q_network es un tensor que contiene los valores Q predichos para cada acción posible en DISCRETE_ACTIONS\n",
    "            #ada valor Q representa la \"calidad\" o \"utilidad\n",
    "            action_idx = torch.argmax(q_values).item()\n",
    "            #devuelve el índice de la acción con el valor Q más alto. Este índice representa la acción que maximiza la recompensa\n",
    "            return DISCRETE_ACTIONS[action_idx]\n",
    "            #lo cual devuelve el vector de acción que se utilizará en el entorno\n",
    "\n",
    "# pues es la que se encarga de entrenar la red neuronal q_network utilizando los datos recolectados en la memoria de experiencias\n",
    "def train_network(q_network, target_network, memory, gamma, batch_size, optimizer):\n",
    "    # Asegurarse de que hay suficiente memoria para entrenar\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    # Seleccionar un batch aleatorio de la memoria\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    # Convertir las transiciones en tensores para el entrenamiento\n",
    "    states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor(dones, dtype=torch.bool, device=device)\n",
    "\n",
    "    # Calcular los valores Q actuales y los valores objetivo para la actualización\n",
    "    q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    #obteniendo un tensor de valores Q predichos para cada acción en cada estado\n",
    "    #agrega una dimensión extra a actions para seleccionar las acciones correctas de los valores Q predichos\n",
    "    #selecciona los valores Q de la red solo para las acciones que el agente realmente tomó en esos estados.\n",
    "    # elimina la dimensión extra creada por gather, para que q_values sea un tensor unidimensional con los valores Q\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_network(next_states).max(1)[0]\n",
    "        # Q futuros esperados el valor Q más alto posible para cada estado en next_states\n",
    "    targets = rewards + gamma * next_q_values * (~dones)\n",
    "    #calcular los valores objetivo se refiere a calcular la \"meta\" o \n",
    "    # \"objetivo\" que queremos que nuestra red neuronal aprenda para cada estado y acción\n",
    "\n",
    "    # Calcular el error cuadrático medio y actualizar la red\n",
    "    loss = nn.functional.mse_loss(q_values, targets)\n",
    "    #Se utiliza la pérdida de error cuadrático medio\n",
    "    #entre los valores Q actuales (q_values) y los valores objetivo (targets).\n",
    "    #La pérdida mide qué tan lejos están las predicciones de q_network de los valores que queremos que aprenda\n",
    "    optimizer.zero_grad()\n",
    "    #borra los gradientes acumulados de la optimización anterior\n",
    "    loss.backward()\n",
    "    #calcula los gradientes de la pérdida con respecto a los parámetros de q_network usando retropropagación\n",
    "    optimizer.step()\n",
    "    #ajusta los pesos de q_network para minimizar la pérdida\n",
    "    #Mejorar la precisión de q_network en predecir valores Q,\n",
    "\n",
    "# Definir la función principal de entrenamiento del agente\n",
    "#Su objetivo principal es permitir que el agente interactúe con el entorno, acumule experiencias, y mejore su red neuronal q_network\n",
    "def train(episodes, max_steps=500, batch_size=64, gamma=0.99, epsilon_decay=0.995, epsilon_min=0.01, render=False):\n",
    "    # Crear el entorno CarRacing en modo de renderizado\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode='human')\n",
    "    input_shape = (env.observation_space.shape[2],) + env.observation_space.shape[:2]\n",
    "    #que será la forma de las imágenes del entorno que se usará como entrada de q_network\n",
    "    num_actions = len(DISCRETE_ACTIONS)\n",
    "    \n",
    "    \n",
    "    # Crear las redes Q y de objetivo y sincronizar pesos\n",
    "    q_network = QNetwork(input_shape, num_actions).to(device)\n",
    "    target_network = QNetwork(input_shape, num_actions).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "    epsilon = 1.0\n",
    "    memory = deque(maxlen=100000)\n",
    "    reward_per_episode = np.zeros(episodes)\n",
    "    \n",
    "    # Bucle principal de entrenamiento por episodio\n",
    "    for episode in range(episodes):\n",
    "        if (episode+1) % 1 == 0:\n",
    "            env.close()\n",
    "            env = gym.make(\"CarRacing-v3\", render_mode=\"human\")\n",
    "        else:\n",
    "            env.close()\n",
    "            env = gym.make(\"CarRacing-v3\")\n",
    "\n",
    "        state, _ = env.reset(seed=42)\n",
    "        state = np.transpose(state, (2, 0, 1))\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        print(f\"Comenzando episodio {episode + 1}\")\n",
    "\n",
    "        # Bucle de pasos dentro de cada episodio\n",
    "        while not done and step_count < max_steps:\n",
    "            action = choose_action(state, q_network, epsilon)\n",
    "            action_idx = DISCRETE_ACTIONS.index(action)\n",
    "            next_state, reward, done, _, _ = env.step(np.array(action, dtype=np.float32))\n",
    "            #toma una accion y ejecuta un paso \n",
    "            \n",
    "            # Añadir recompensa si el agente acelera y penalizar giros fuertes\n",
    "            if action[1] > 0:\n",
    "                reward += 1.0  # Recompensa adicional por acelerar\n",
    "            if reward > 0 and action_idx == 2:\n",
    "                reward += 0.5\n",
    "            elif action_idx in [0, 4]:\n",
    "                reward -= 5\n",
    "\n",
    "            next_state = np.transpose(next_state, (2, 0, 1))\n",
    "            # Reorganaizar las dimensiones de la observación del entorno\n",
    "            memory.append((state, action_idx, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            train_network(q_network, target_network, memory, gamma, batch_size, optimizer)\n",
    "            #train_network se llama para entrenar q_network con un batch de experiencias de \n",
    "            # memory, ajustando los pesos de q_network en función de los valores objetivo\n",
    "        \n",
    "        # Reducir epsilon gradualmente para favorecer la explotación\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        #Se reduce para que el agente explore menos con el tiempo\n",
    "        reward_per_episode[episode] = episode_reward\n",
    "\n",
    "        print( f\"Recompensa del episodio : {episode_reward}\")\n",
    "\n",
    "        # Actualizar los pesos de la red de objetivo cada 10 episodios\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "            #e actualiza copiando los pesos de q_network. \n",
    "            # Esto estabiliza el entrenamiento al reducir la varianza en los valores Q\n",
    "\n",
    "    # Graficar el desempeño del agente en términos de recompensa acumulada por episodio\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.xlabel(\"Episodios\")\n",
    "    plt.ylabel(\"Recompensa acumulada\")\n",
    "    plt.title(\"Desempeño del agente durante el entrenamiento\")\n",
    "    plt.show()\n",
    "\n",
    "# Llamar a la función de entrenamiento con 1000 episodios\n",
    "train(episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
